{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Multi-class Classification with Logistic Regression\n",
    "</h1><br>\n",
    "\n",
    "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9). The recognition of hand-written digits is a multi-class classification problem with $k = 10$ classes, i.e $y^i \\in \\{1, 2, 3, ..., 10\\}$. As a result, to use logistic regression, we have to fit 10 classifiers using  **one-vs-all** technique. Therefore, for each class we'll build a classifier where that class would be the *positive* class and all other classes would be the *negative* class. After fitting the 10 classifiers, we use the 10 models in predicting each training example $x^i$ and pick the class that has the highest probability from those 10 models; i.e would end up with column vector $h_\\theta(x^i) \\in \\mathbb{R}^k$.\n",
    "\n",
    "Let's get started by imporitng the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up notebook global style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat, whosmat # load Matlab files and inspect objects in those files before loading them\n",
    "import scipy.optimize as opt\n",
    " \n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Functions\n",
    "</h2><br>\n",
    "We'll write the functions needed to solve regularized logistic regression and implement Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid fn\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Compute cost of regularized logistic regression\n",
    "def cost_reg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: cost with shrinkage penalty added --> scalar\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z) # hypothesis\n",
    "    error = np.multiply(y, np.log(hyp)) + np.multiply(1 - y, np.log(1 - hyp))\n",
    "    shrinkage_penalty = (lambda_ /(2 * m)) * (np.power(theta[1:, :], 2))\n",
    "    cost = - (1 / m) * np.sum(error) + np.sum(shrinkage_penalty)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "# Compute the gradients of regularized logistic regression\n",
    "def gradient_reg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: gradient array\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z)\n",
    "    error = hyp - y\n",
    "    grad = (1 / m) * (X.T.dot(error))\n",
    "    grad[1:, :] = grad[1:, :] + (lambda_ / m) * theta[1:, :]\n",
    "    \n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "# Define one-vs-all k classifier\n",
    "def one_vs_all(X, y, num_classes, lambda_):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    num_classes: scalar; number of classes at hand\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: theta matrix k x (n + 1) where each row is a classifier and each column is a featute.\n",
    "            i.e. each cell is a fauture weight for the classifier on the same row.\n",
    "    '''\n",
    "    # Initializes zero theta parameter and theta matrix\n",
    "    init_theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Theta matrix will store theta parameters for all k models\n",
    "    theta_matrix = np.zeros((num_classes, X.shape[1]))\n",
    "    \n",
    "    for i in range(1, num_classes + 1):\n",
    "        # Would be 1 for the i class and 0 for all ther classes i.e one-vs-all\n",
    "        y_i = (y == i) * 1 \n",
    "        \n",
    "        # Compute theta\n",
    "        result = opt.fmin_tnc(\n",
    "            func = cost_reg, x0=init_theta, args=(X, y_i, lambda_), fprime=gradient_reg)\n",
    "        \n",
    "        # Store theta parameters array in theta matrix\n",
    "        theta_matrix[i - 1] = result[0]\n",
    "    \n",
    "    return theta_matrix\n",
    "\n",
    "\n",
    "# Define predict fn for one-vs-all\n",
    "def predict_one_vs_all(theta, X):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    theta: theta matrix k x (n + 1\n",
    "    Return: predictions column vector m x 1\n",
    "    '''\n",
    "    pred = X.dot(theta.T) # will leave us with matrix m * k where each column represents the logit\n",
    "    hyp = sigmoid(pred)\n",
    "    pred = (hyp.argmax(axis=1) + 1) # it will give us the index of the highest probability\n",
    "                                      # per row (column index) which will allow us to assign\n",
    "                                      # the class for the training example\n",
    "    return pred.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Define accuracy fn\n",
    "def accuracy(predictions, y):\n",
    "    '''\n",
    "    predictions: predictions' class\n",
    "    y: Actual class for each m examples --> m x 1 column vector\n",
    "    Return: Accuracy rate --> scalar\n",
    "    '''\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    return f\"accuracy = {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Regularized Logistic Regression: Multi-class Classification\n",
    "</h2><br>\n",
    "Below is a refresher from logistic regression notebook.\n",
    "\n",
    "The hypothesis function is:\n",
    "$$P(y^i = 1/X,\\theta) = h_\\theta(X^i) = \\frac {1} {1+ \\exp^{-\\theta^{T}X^i}}$$\n",
    "\n",
    "$$\\Rightarrow\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = \\exp^{\\theta^{T}X^i}$$\n",
    "\n",
    "$$\\Rightarrow log\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = {\\theta^{T}X^i}$$\n",
    "\n",
    "The cost function will be:\n",
    "$$J(\\theta) = - \\frac {1} {m} \\sum_{i = 1}^{m} \\big\\{ y^ilog(h_\\theta(X^i)) + (1 - y^i)log(1 - h_\\theta(X^i))\\big\\} + \\frac {\\lambda} {2m} \\sum_{j = 1}^{n}\\theta_j^2$$\n",
    "Since we don't apply the regularization on the bias (intercept); therefore, we'll have two update equations for the gradient descent algorithm:\n",
    "$$\\theta_0 = \\theta_0 - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_0^i$$\n",
    "$$\\theta_j = \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_j^i\\quad;\\,j = 1, 2 , ..., n$$\n",
    "\n",
    "Finally, since it's *one-vs-all* classification, we'll have k hypothesis for each example:\n",
    "Train a logistic regression classifier $h_\\theta^i(X)$ for each class $i$ to predict the probability that $y = i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes we have: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Below is a sample of 20 images of the digits we have:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "data = loadmat('../data/ex3data1.mat')\n",
    "y = data['y']\n",
    "X = data['X']\n",
    "\n",
    "# Get the number of classes we have using unique method\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"The unique classes we have: {np.unique(y)}\")\n",
    "\n",
    "# randomly select 20 images to show them\n",
    "sample = np.random.choice(X.shape[0], 20)\n",
    "print(\"Below is a sample of 20 images of the digits we have:\")\n",
    "plt.imshow(X[sample, :].reshape(-1, 20).T)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matlab file contains two objects: \n",
    "* $X$: feature matrix with 400 features. Each example is a 20 pixels x 20 pixels image $\\rightarrow$ 400 features.\n",
    "* $y$: target variable. Note that 0 is 10 in the data for indexing purposes.\n",
    "* $m = 5000$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = 96.46%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding columns of ones for the intercept\n",
    "X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "\n",
    "# Run the OneVsAll to get all models\n",
    "lambda_ = 0.1\n",
    "all_theta = one_vs_all(X, y, num_classes, lambda_)\n",
    "\n",
    "# Predict the training examples class\n",
    "pred = predict_one_vs_all(all_theta, X)\n",
    "accuracy(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using 10 clasifiers is close to 96.46%. We could've tried adding some non-linearities to the model as well as tuning the hyperparameter $\\lambda$ using cross validation to get the most out of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2><br>\n",
    "A few takeaway points:\n",
    "* We didn't tune shrinkage parameter to pick the soft spot of *bias-variance* trade-off that may've helped us in improving the accuracy rate.\n",
    "    * We could've also add non-linearity to the features but it may become very complex so fast since adding $2^{nd}$ degree polynomial will grow at ${O}(n^2)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
